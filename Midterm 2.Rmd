---
title: "Math 181B Midterm 2"
output:
  html_document: default
  pdf_document: default
---

# Midterm 2

## Question 1.
##### 1) Find E[Y_i]:
_since epsilon_i is the only random variable in the simple regression model,_
  $$
  \begin{aligned}
  Y_i &= \beta_0 + \beta_1 x_i + \epsilon_i \\
  E[\epsilon_i] &= 0 \\
  E[Y_i] &= E[\beta_0 + \beta_1 x_i + \epsilon_i] \\
         &= \beta_0 + \beta_1 x_i + E[\epsilon_i] \\
         &= \beta_0 + \beta_1 x_i
  \end{aligned}
  $$

##### 2) Find Var(Y_i):
_since epsilon_i is the only random variable in the simple regression model, according to the variance property: Var[ax + b] = a^2 Var[x]:_
  $$
  \begin{aligned}
  Y_i &= \beta_0 + \beta_1 x_i + \epsilon_i \\
  Var(\epsilon_i) &= \sigma^2 \\
  Var[Y_i] &= Var(\beta_0 + \beta_1 x_i + \epsilon_i) \\
           &= Var(\epsilon_i) \\
           &= \sigma^2 \\
  \end{aligned}
  $$
##### 3) find Cov(Y_i, Y_j) for (i!=j) :
_if i != j, then Cov(Y_i, Y_j) != Cov(Y_i, Y_i) != Cov(Y_j, Y_j). Since all epsilons are i.i.d. from each other, therefore: each Y_i and Y_j with i!=j are independent. Moreover, for i!=j, epsilon_i and epsilon_j are 0. In the Sample regression model, Cov(Y_i, Y_j) != 0 only if i==j._ 
  $$
  \begin{aligned}
  Cov(Y_i, Y_j) &= 0 \space for \space i \neq j \\
  \end{aligned}
  $$
##### 4) find unbiased estimator: 
  $$
  \begin{aligned}
  \hat \beta_0 &= \bar y - \hat \beta \bar x \\
  \hat \beta_1 &= \frac{\sum_{i=1}^n (x_i - \bar x)(y_i - \bar y)}{\sum_{i=1}^n (x_i - \bar x)^2} \\
  \hat \sigma^2 &= \frac{1}{n-2} \sum_{i=1}^n (y_i - \hat y_i)^2 \\
  \end{aligned}
  $$
##### 5) show: 
  $$
  \begin{aligned}
  \bar Y &= \frac{1}{n} \sum_{i=1}^n Y_i \\
  \hat Y_i &= \hat \beta_0 + \beta_1 x_i \\
  \sum_{i=1}^n (Y_i - \bar Y)^2 &= \sum_{i=1}^n (Y_i^2 - 2Y_i \bar Y + \bar Y^2) \\
                                &= \sum_{i=1}^n Y_i^2 -2\sum_{i=1}^n Y_i * \bigl(\frac{1}{n}\sum_{i=1}^n Y_i\bigr) + (\frac{1}{n} \sum_{i=1}^n Y_i)^2 \\
                                &= 
                                
  \end{aligned}
  $$

## Question 2.
##### a) 
```{r}
x <- c(4.4, 3.9, 4.0, 4.0, 3.5, 4.1, 2.3, 4.7, 1.7, 4.9, 1.7, 4.6, 3.4, 4.3, 1.7, 3.9, 3.7, 3.1, 4.0, 1.8, 4.1, 1.8, 3.2, 1.9, 4.6, 2.0, 4.5, 3.9, 4.3, 2.3, 3.8, 1.9, 4.6, 1.8, 4.7, 1.8, 4.6, 1.9, 3.5, 4.0, 3.7, 3.7, 4.3, 3.6, 3.8, 3.8, 3.8, 2.5, 4.5, 4.1, 3.7, 3.8)
y <- c(78, 74, 68, 76, 80, 84, 50, 93, 55, 76, 58, 74, 75, 80, 56, 80, 69, 57, 90, 42, 91, 51, 79, 53, 82, 51, 76, 82, 84, 53, 86, 51, 85, 45, 88, 51, 80, 49, 82, 75, 73, 67, 68, 86, 72, 75, 75, 66, 84, 70, 79, 60) 
n <- 52
x.rand <- rep(NA, 20)
y.rand <- rep(NA, 20)
(idx.rand <- sample(1:n, 20))
set.seed(5)
for (i in 1:20) {
  x.rand[i] = x[idx.rand[i]]
  y.rand[i] = y[idx.rand[i]]
}
print(x.rand)
print(y.rand)
lm <- lm(y.rand ~ x.rand)
summary(lm)
```
  $$
  \hat \beta_0 = 35.369 \\
  \hat \beta_1 = 9.842 \\
  $$

##### b) 
  $$
  H_0: \beta_1 = 0 \\
  H_1: \beta1 \neq 0 \\
  $$
```{r}
lm <- lm(y.rand ~ x.rand)
summary(lm)
p.value <- 2.48e-06
alpha <- 0.05
p.value < alpha
```
_suppose the level of significant is 0.05, since the p-value is 0.00000248, which is very small and less than 0.05. Therefore we reject H0._

##### c)
  $$
  \beta_0 \in (\hat \beta_0 - t_{n-2}*\frac{\alpha}{2}*SE[\hat \beta_0], \space \hat \beta_0 + t_{n-2}*\frac{\alpha}{2}*SE[\hat \beta_0]) \\
  $$
```{r}
alpha <- 1 - 0.95
n <- 20
(t.crit <- qt(1 - alpha/2, df = n-2))
beta0.hat <- 35.369
SE.beta0 <- 5.080
(CI.low <- beta0.hat - t.crit*SE.beta0)
(CI.upp <- beta0.hat + t.crit*SE.beta0)
```

##### d)
```{r}
summary(lm(y.rand~x.rand))
```
_the r.sq is 0.7173. this r.sq is the proportion of the total variation in each of the interval to the next eruption that can be attributed to the linear relationship with duration of an eruption._

##### e)
```{r}
lm.resid <- resid(lm(y.rand ~ x.rand))
plot(x.rand, lm.resid, ylab = "Residuals", xlab = "x")
abline(0,0, col = 2)
hist(lm.resid)
```
_according to the histogram of residuals, we observed that the path is not normally distributed and the has the fatter tail on the left side, which the path is considered t-distributed. Moreover, there's a big U-shape in the scatter plot, which the scatter plot shows many patterns and is non-random pattern. Therefore, the linear model is not appropriate._

##### f)
```{r}
summary(lm(y.rand ~ x.rand))
```
_since the s^2 is the unbiased estimator of sigma.sq, which means:
  $$
  \hat {\sigma^2} = s^2 \\
  $$
```{r}
s <- 7.542
(s.sq <- s^2)
```
_therefore, the sigma.sq.hat is 56.88176._

##### g)
```{r}
predict(lm(y.rand~x.rand), newdata = data.frame(x.rand=4.0), interval = 'confidence', level = 0.95)
```
*the 95% confidence interval for E(Y|X = 4) is: (70.58193, 78.89409).*


## Question 3.
```{r}
df = data.frame(data = c(14.29, 19.10, 19.09, 16.25, 15.09, 16.61, 19.63, 20.06, 20.64, 18.00, 19.56, 19.47, 19.07, 18.38, 20.04, 26.23, 22.74, 24.04, 23.37, 25.02, 23.27), methods = rep(c("A", "B", "C"), each = 7))
df
```
##### a)
  $$
  H0: \mu_A = \mu_B = \mu_C \\
  H1: at \space least \space one \space \mu_i \space \neq 0 \space for \space i \in [A,B,C] \\
  $$
```{r}
aov.fit <- aov(data ~ methods, data = df)
summary(aov.fit)
```
_assume the level of significant is 0.05. Since the p.value is 0.00000807, which is very small and less than 0.05. Therefore, we reject H0._

##### b)
  $$
  H0: \mu_A = \mu_B \\
      \mu_A - \mu_B = 0 \\
  H1: \mu_A \neq \mu_B \\ 
  $$
```{r}
df = data.frame(data = c(14.29, 19.10, 19.09, 16.25, 15.09, 16.61, 19.63, 20.06, 20.64, 18.00, 19.56, 19.47, 19.07, 18.38, 20.04, 26.23, 22.74, 24.04, 23.37, 25.02, 23.27), methods = rep(c("A", "B", "C"), each = 7))
df
df$methods = as.factor(df$methods)
AvsB = c(1,-1, 0)
Matrix = cbind(AvsB)
Clist = list("AvsB" = 1)
contrasts(df$methods) = Matrix
aov.model = aov(data ~ methods, data = df)
summary(aov.model,split=list(methods=Clist))
```
_assume the level of significant is 0.05. Since the p.value is 0.0328, which is less than 0.05. Therefore we reject H0._

##### c)
  $$
  H0: \mu_A = \mu_B \\
  H1: \mu_A \neq \mu_B \\
  $$
```{r}
df = data.frame(data = c(14.29, 19.10, 19.09, 16.25, 15.09, 16.61, 19.63, 20.06, 20.64, 18.00, 19.56, 19.47, 19.07, 18.38, 20.04, 26.23, 22.74, 24.04, 23.37, 25.02, 23.27), methods = rep(c("A", "B", "C"), each = 7))
n <- 6
m <- 6
(A <- df$data[1:7])
(B <- df$data[8:14])
t.test(A, B, alternative = "two.sided", paired = FALSE, conf.level = 0.95)
```
_assume the level of significant is 0.05. Since the p.value is 0.03859, which is less than 0.05. Therefore, we reject H0. The results in b) and c) are very closed, which means the contract test is basically the unpaired 2-sample t-test._ 

## Question 4.

##### a)
_I will use two-way ANOVA. Because in this table, each data was located by both treatments and blocks, which for data within a treatment, they cannot switch with each other. Each of them has their specific block/location._

##### b)
```{r}
df = data.frame(data = c(3.20, 3.03, 3.47, 3.71, 4.07, 3.33, 3.19, 3.79, 3.38, 3.10, 3.13, 3.13, 3.04, 3.14, 3.00, 3.20))
df$Lang <- c("Samoan", "Tongan", "Hawaiian", "Pacific Island English", "Maori", "Mandarin", "Cantonese", "Fijian")
df$Gender <- rep(c("Male", "Female"), each = 8)
df
summary(aov(data ~ Lang+Gender, data = df))
```
_assume the level of significant is 0.05. since the p.value for comparing different genders is 0.0472, which is less than 0.05. Therefore, we reject H0._


##### c)
_I will use pair t-test. Because this is a two-way ANOVA table, which each of data are pairwise depending on their own treatment and blocks. This design is so-called randomized Block Design model._

##### d)
```{r}
x <- df$data[1:8]
y <- df$data[9:16]
t.test(x, y, alternative = "two.sided", paired = TRUE)
```
_the p.value is 0.04719. Comparing it with the assumed level of significant 0.05, it is lower than 0.05. Therefore, we reject H0._

##### e)
_the result in b) is 0.0472, and the result in d) is 0.04719, which these two results are nearly equal. Therefore, we can conclude that two-way ANOVA is similar to the paired t-test._


## Question 5. 

##### a)
```{r}
hemoglobin <- c(6.7, 7.8, 5.5, 8.4, 7.0, 7.8, 8.6, 7.4, 5.8, 7.0, 7.0, 7.8, 6.8, 7.0, 7.5, 6.5, 5.8, 7.1, 6.5, 5.5, 9.9, 8.4, 10.4, 9.3, 10.7, 11.9, 7.1, 6.4, 8.6, 10.6, 9.9, 9.6, 10.2, 10.4, 11.3, 9.1, 9.0, 10.6, 11.7, 9.6, 10.4, 8.1, 10.6, 8.7, 10.7, 9.1, 8.8, 8.1, 7.8, 8.0, 9.9, 9.6, 10.4, 10.4, 11.3, 10.9, 8.0, 10.2, 6.1, 10.7, 9.3, 9.3, 7.8, 7.8, 9.3, 10.2, 8.7, 8.6, 9.3, 7.2, 11.0, 9.3, 11.0, 9.0, 8.4, 8.4, 6.8, 7.2, 8.1, 11.0)
Rate <- rep(c("1","2","3","4"), each = 20)
Method <- rep(rep(c("A", "B"), each=10),4)
hemoglobin
fish = data.frame(data = hemoglobin, Rate = Rate, Method = Method)
interaction.plot(Rate, Method, hemoglobin)
interaction.plot(Method, Rate, hemoglobin)
```
##### b)
```{r}
summary(aov(data ~ Rate+Method, data = fish))
```

##### c)
```{r}
summary(aov(data ~ Rate*Method, data = fish))
```
_since the p.value for interaction effect is 0.363, which is large and greater than the level of significant 0.05. Therefore, we fail to reject H0. Moreover, the p.value for Rate effect is 1.87 * 10^-9, which is small enough to show that: for different rates, the means of hemoglobin are not equal. Since the p.value for Method effect is 0.227, it also tells that: for different methods, the means of hemoglobin are equal._


##### d)
_the result in b) and c) has the same p.value for Method effect. Even though the p.value for Rate effect were different, they are very closed to each other, which tells that the interaction effect doesn't affect the Methods effect, which the Method is independent._
